{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction Step : Keywords "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relevant formulas and load language models. \n",
    "def clean_french_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in french_stopwords]\n",
    "    return ' '.join(words)\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_md\")\n",
    "french_stopwords = set(stopwords.words(\"french\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of list of Keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extraction of keywords was done on base of the datalist of death during police interventions collected manually by Border Forensics. However, this list isn't public. Therefore, you can't find the excel on github. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset of Border Forensics\n",
    "df = pd.read_excel(\"../NotInDatabase/Border_Forensics_Database.xlsx\")  \n",
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the French text of it\n",
    "df[\"text\"] = df[\"Titre\"]+\". \" + df[\"Lead_posts\"]\n",
    "df_fr = df[(df[\"Main language\"] == \"Fr\")]\n",
    "\n",
    "text = ' '.join(df_fr['text'].dropna().apply(clean_french_text))\n",
    "doc = nlp(text)\n",
    "\n",
    "words = [token.lemma_.lower() for token in doc if not token.is_punct and not token.is_space]\n",
    "\n",
    "# We use counter to see the importance of the different words in the text (how many times they are occuring)\n",
    "word_freq = Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity thresholds of 0.4 for \"police\" and 0.5 for \"death\" were selected based on manual inspection of the resulting word lists at various cutoff values. These thresholds appeared to produce the most relevant list of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Temp\\ipykernel_12620\\3214182339.py:3: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if nlp(word).similarity(police_ref) > 0.4}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'policier': 7, 'instruction': 1, 'police': 11, 'agent': 1, 'défense': 2, 'accusation': 1, 'victime': 1, 'procureur': 2, 'violence': 2, 'fonctionnaire': 1, 'réclusion': 1, 'intervention': 2, 'garde': 2, 'justice': 1, 'militaire': 1, 'prison': 1, 'administratif': 1, 'manifestation': 1, 'interpellation': 1, 'arrestation': 1, 'frontière': 1, 'ministère': 1, 'tribun': 1, 'commissariat': 1, 'affaire': 1, 'plainte': 2, 'enquête': 1, 'secour': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stefa\\AppData\\Local\\Temp\\ipykernel_12620\\3214182339.py:8: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  if nlp(word).similarity(mort_ref) > 0.5}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rafle': 1, 'mort': 10, 'décéder': 4, 'décès': 2, 'prison': 1, 'mourir': 2}\n"
     ]
    }
   ],
   "source": [
    "# Extract the words with similarity higher than 0.4 to the word \"police\"\n",
    "police_ref = nlp(\"police\")\n",
    "police_related_words = {word: count for word, count in word_freq.items()\n",
    "                        if nlp(word).similarity(police_ref) > 0.4}  \n",
    "print(police_related_words)\n",
    "\n",
    "# Extract the words with similarity higher than 0.5 to the word \"death\"\n",
    "mort_ref = nlp(\"mort\")\n",
    "mort_related_words = {word: count for word, count in word_freq.items()\n",
    "                        if nlp(word).similarity(mort_ref) > 0.5} \n",
    "print(mort_related_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The handmade list was made in order ot complete the lists extracted by the texts. It is based on words that were often occuring in the discussion with the expert, can however be completet if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_handmade = ['tuer', 'cellule', 'arrest', 'officier',  'garde à vue',  'balles', 'tir', 'coup de feu', 'acquitt', 'légitime défense', 'classement', 'plaquage ventral', 'contrôle d’identité', 'détention', 'forces de l’ordre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classement', 'cellule', 'manifestation', 'accusation', 'police', 'garde', 'commissariat', 'affaire', 'garde à vue', 'arrestation', 'légitime défense', 'interpellation', 'forces de l’ordre', 'instruction', 'plaquage ventral', 'tribun', 'policier', 'balles', 'tir', 'réclusion', 'tuer', 'prison', 'mourir', 'décéder', 'justice', 'victime', 'fonctionnaire', 'militaire', 'ministère', 'violence', 'administratif', 'décès', 'coup de feu', 'détention', 'intervention', 'officier', 'agent', 'défense', 'frontière', 'acquitt', 'rafle', 'procureur', 'secour', 'enquête', 'arrest', 'plainte', 'mort', 'contrôle d’identité'}\n"
     ]
    }
   ],
   "source": [
    "list_keywords = set(list(police_related_words.keys()) + list(mort_related_words.keys()) + lst_handmade)\n",
    "print(list_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction on those keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the list of border Forensics is confidential, the set of keywords extracted with the code above has been copied direclty inside the code in order for it to work as well in absence of the database of Border Forensics that worked as reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_keywords = {'administratif', 'interpellation', 'procureur', 'classement', 'tir', 'agent', 'défense', 'mourir', 'décès', 'accusation', 'mort', 'tuer', 'garde à vue', 'enquête', 'intervention', 'militaire', 'détention', 'acquitt', 'justice', 'forces de l’ordre', 'réclusion', 'tribun', 'plainte', 'police', 'arrestation', 'commissariat', 'officier', 'balles', 'violence', 'policier', 'affaire', 'coup de feu', 'arrest', 'contrôle d’identité', 'fonctionnaire', 'légitime défense', 'instruction', 'manifestation', 'garde', 'secour', 'frontière', 'décéder', 'plaquage ventral', 'prison', 'rafle', 'victime', 'ministère', 'cellule'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policier_lethal(title, set_keywords):\n",
    "    doc = nlp(title.lower())\n",
    "    keywords = set_keywords\n",
    "    has_police = any(any(kw in token.text for kw in keywords) for token in doc)\n",
    "    return has_police"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scratched Titles\n",
    "df = pd.read_excel(\"../databases/Titles_Le_Temps.xlsx\")  \n",
    "df = df[[\"Title\", \"Link\", \"Date\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({'Title': 'string'})\n",
    "df['Datetime'] = pd.to_datetime(df['Date'].astype(str), format='%Y%m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_police'] = False\n",
    "df['is_police'] = df['Title'].apply(lambda x: policier_lethal(x, set_keywords))\n",
    "\n",
    "df_pl = df[df['is_police'] == True]\n",
    "df_pl.to_excel(\"../databases/Keyword_extracted.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Lead Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The principal of the data scratching was the same as for the titles. In order to make the system a bit strongenr, a sleeping rate has been introduced and if it wasn't working, the code was trying several times to scratch the Lead Post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I think its not the right version\n",
    "links = df_pl['Link'].to_list()\n",
    "data = []\n",
    "missed_data = []\n",
    "\n",
    "for i in links:\n",
    "    url = i\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    success = False\n",
    "    retries = 0\n",
    "    max_retries = 5  # prevent infinite loops\n",
    "\n",
    "    while not success and retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                print(f\"Rate limited (429) on {url}, retrying...\")\n",
    "                time.sleep(1)\n",
    "                retries += 1\n",
    "                continue\n",
    "\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            post_lead = soup.find(class_=\"post__lead\")\n",
    "            if post_lead:\n",
    "                articles = post_lead.select(\"p\")\n",
    "                for article in articles:\n",
    "                    lead_post = article.get_text(strip=True)\n",
    "\n",
    "                    data.append({\"Title\": lead_post, \"Link\": i})\n",
    "            else:\n",
    "                print(f\"No 'lead_post' section found on {url}\")\n",
    "\n",
    "            success = True  # Success: exit loop\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}, retrying...\")\n",
    "            time.sleep(1)\n",
    "            retries += 1\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed after retries: {url}\")\n",
    "        missed_data.append(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the dataframes can be exportet to Excels to keep track of the intermediate steps. This step needed to be repeated several times by replacing 'links' by the links inside the 'missed_data' and merging afterwards the two dataframes. The server couldn't handle that many requests. At then end, we still had  408 / 35553 Titles where no Lead Posts could be attributed. However, some of the articles didn't have Lead Posts, why there couldn't be no Lead Post extracted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data, columns= [\"Title\", \"Link\"])\n",
    "missed_data = pd.DataFrame(data=missed_data, columns= [\"Link\"])\n",
    "df.head()\n",
    "\n",
    "df = df[['Title', 'Link', 'Datetime', 'Post_Lead', ]]\n",
    "df.to_excel(\"databases/LeadPosts_Le_Temps.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ada",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
