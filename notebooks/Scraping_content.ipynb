{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dab80f36",
   "metadata": {},
   "source": [
    "# Scraping content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022c3b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import relativedelta\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a06a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read database\n",
    "df_outcomes = pd.read_excel(\"../databases/Pipeline2_Outcome.xlsx\")\n",
    "links = df_outcomes[\"link\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ec27ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape content\n",
    "data = []\n",
    "missed_data = []\n",
    "\n",
    "for i in links:\n",
    "    url = i\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "\n",
    "    success = False\n",
    "    retries = 0\n",
    "    max_retries = 5  # prevent infinite loops\n",
    "\n",
    "    while not success and retries < max_retries:\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            \n",
    "            if response.status_code == 429:\n",
    "                print(f\"Rate limited (429) on {url}, retrying...\")\n",
    "                time.sleep(1)\n",
    "                retries += 1\n",
    "                continue\n",
    "\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "            post_content = soup.find(class_=\"post__content\")\n",
    "            content_text = []\n",
    "\n",
    "            if post_content:\n",
    "                articles = post_content.find_all(\"p\", recursive=False)\n",
    "                for article in articles:\n",
    "                    post_content = article.get_text(strip=True)\n",
    "\n",
    "                    content_text.append(post_content)\n",
    "\n",
    "            post_body = soup.find(class_=\"post__body\")\n",
    "            if post_body:\n",
    "                articles = post_body.find_all(\"p\", recursive=False)\n",
    "                for article in articles:\n",
    "                    post_body = article.get_text(strip=True)\n",
    "\n",
    "                    content_text.append(post_body)\n",
    "            else:\n",
    "                print(f\"No 'content' section found on {url}\")\n",
    "            data.append({\"Content\": content_text, \"Link\": url})\n",
    "\n",
    "            success = True  # Success: exit loop\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {e}, retrying...\")\n",
    "            time.sleep(1)\n",
    "            retries += 1\n",
    "\n",
    "    if not success:\n",
    "        print(f\"Failed after retries: {url}\")\n",
    "        missed_data.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a7eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=data, columns= [\"Content\", \"Link\"])\n",
    "\n",
    "df.to_excel(\"shelfens/Pipeline2_Matched_dataset.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
